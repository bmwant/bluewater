\section{Аналіз рішення}
З практичної точки зору недоцільно розглядати лише математичний апарат запропонованого методу, оскільки корисність та необхідність використання алгоритму на реальних вхідних даних вимагає емпіричного підтвердження чи порівняння роботи з уже готовими рішеннями. Оскільки розроблюваний метод є апроксимацією довільно обраної моделі, було проведено порівняння з найкращою моделлю (що надалі була взята за базову) на основі алгоритму Support Vector Machines, а також був проведений аналіз для вибірки датасетів. Всі результати були зіставлені з визначеними заздалегідь нефункціональними вимогами для підтвердження відповідності програмного продукту поставленим критеріям.

\subsection{Порівняльний аналіз}
Було проведено порівняльний аналіз рішення з існуючими реалізаціями для підтердження ефективності використання даного алгоритму. Обрані такі критерії для побудови порівняльної таблиці:

\begin{itemize}
	\item відхилення від еталонної величини;
	\item середнє квадратичне відхилення;
	\item час роботи на рядок вхідних даних датасету.
\end{itemize}

Також для найкращої моделі та гібридної моделей були побудовані таблиці помилок (\textit{confusion matrix}) - матриці, що допомагають візуалізувати ефективність алгоритмів. Кожна колонка містить кількість результатів в передбачуваному класі, в той час як кожен рядок містить дійсну кількість елементів у класі (рис. \ref{fig:confusion_matrix}).

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{figures/confusion_matrix.png}
  \caption{Приклад матриці помилок для вхідного набору даних Iris}
  \label{fig:confusion_matrix}
\end{figure}

Результати для моделі \textit{Support Vector Machines} та побудованої гідбридної моделі (табл. \ref{tab:confusion_matrix}) дають змогу зрозуміти, обидві моделі відносять елементи до однакових класів. Це означає, що хоч і точніть передбачення не є максимально можливою, проте дозволяє показати стабільні консистенті результати для обох моделей, а це, у свою чергу, дозволяє підтверджувати надійність розроблюваного підходу.

\begin{table}[h!]
	\begin{tabularx}{\textwidth}{|X|X|X|X|}
    \hline
     & setosa & versicolor & virginica \\ \hline
    setosa & 7 & 0 & 0 \\ \hline
    versicolor & 0 & 10 & 2 \\ \hline
    virginica & 0 & 0 & 11 \\
    \hline
    \end{tabularx}
\caption{Матриця помилок} \label{tab:confusion_matrix}
\end{table}

\subsection{Відповідність поставленим критеріям}
Для перевірки відповідності висунутим вимогам було проведено аналіз для кожного окремо висунотого пункту нефункціональних вимог. Для перевірки швидкодії було обрано 5 різних різнорідних вхідних наборів даних (\ref{tab:datasets_performance}).

\begin{table}[h!]
	\begin{tabularx}{\textwidth}{|c|X|X|X|X|}
    \hline
    № п/п & Назва & Розмір & Час (мс на рядок) & Точність (validation) \\ \hline
    1 & creditcard.csv & 68.08 MB & ~0.02 & 0.9833 \\ \hline
    2 & movie\_metadata.csv & 579.78 KB & ~0.01 & 0.9754 \\ \hline
    3 & mushrooms.csv & 365.24 KB  & ~0.01 & 0.9666 \\ \hline
    4 & 77\_cancer\_proteomes\_CPTAC\_itraq.csv & 5.41 MB & ~0.01 & 0.9342 \\ \hline
    5 & menu.csv & 29.29 KB & ~0.007 & 0.9303 \\
    \hline
    \end{tabularx}
\caption{Швидкодія роботи на різних вхідних даних} \label{tab:datasets_performance}
\end{table}

Було обрано різні датасети розміром від декількох кілобайт до десятків мегабайт для перевірки поведінки моделі під час роботи з різними об'ємами даних. Результати показали, що зі збільшенням об'єму даних, час роботи алгоритму росте лінійно, тобто залишається сталим час на обробку одного рядку вхідних даних. Це означає, що модель теоретично готова приймати необмеженого розміру дані на вхід (зрозуміло, спираючись на обмеження оперативної пам'яті та дискового простору).

Для перевірки точності було обраховано середнє квадратичне відхилення, щоб дізнатися, на скільки в середньому відхиляються індивідуальні значення передбачуваної величини від її середнього значення (\ref{tab:standard_deviation}).

\begin{table}[h!]
	\begin{tabularx}{\textwidth}{|c|X|X|}
    \hline
    № п/п & Назва алгоритму & Значення відхилення \\ \hline
    1 & Logistic regression & 0.04082 \\ \hline
    2 & Linear Discriminant Analysis & 0.03818 \\ \hline
    3 & Gaussian Naive Bayes & 0.05335 \\ \hline
    4 & Support Vector Machines & 0.02511 \\ \hline
    5 & Decision Tree Classifier & 0.04064 \\ \hline
    6 & K-nearest neighbors & 0.03333 \\
    \hline
    \end{tabularx}
\caption{Середнє квадратичне віхилення для досліджуваних алгоритмів} \label{tab:standard_deviation}
\end{table}

Порівняння дає змогу зрозуміти, який з алгоритмів менше за все схильний до видачі "несподіваних" результатів, тобто таких значень, що будуть давати помітні стрибки досліджуваної величини. Навіть за умови досить точного передбачення існують такі варіанти використання, де відхилення буде відігравати набагато важливішу роль. Тобто, інколи важливіше не виходити за межі допустимих обмежень по всіх даних загалом, ніж отримати більш точні результати для індивідуальних записів. Для даного порівняння алгоритм класу \textit{Support Vector Machines} показав найкращі результати, саме тому гібридна модель для таких початкових даних буде будуватися на основі цієї моделі.

\subsection{Доцільність вибору інструментарію}

\subsection{Отримані результати та шляхи покращення}
Отже, в результаті отримана реалізація показала відповідність усім висунутим вимогам та продемонструвала стабільні показники незалежно від виду та об'єму вхідних даних. На поточному етапі такі результати повністю задоволняють як користувачів алгоритму, так і розробників, які бажають покращити алгоритм чи модифікувати його будь-яким чином для забезпечення кращих показників точності чи швидкодії.

Серед напрямків для покращення алгоритму можна виділити такі основні:

\begin{itemize}
	\item Визначення оптимального алгоритму не шляхом повного перебору існуючих моделей, а за допомогою деякої евристики. Зараз для вхідних даних необхідно побудувати всі моделі, користуючись медотом повного перебору (\textit{brute force}) і виконати їхнє порівняння за деякою обраною метрикою. Лише після цього на основі кращої моделі буде побудована наша гібридна модель. Якщо ж скористатися деяким набором правил, евристикою чи іншими додатковими знаннями в доменній залузі - можна обмежити кількість моделей, що будуть побудовані, таким чином в декілька разів зменшити витрати на час на етапі побудови моделей. Виграш буде значно відчутний на великій кількості моделей за умови, що лише кілька з них дійсно показують стабільно найкращі результати для схожих вхідних даних. Однією із можливих реалізацій такого прийому може бути додаткова модель-класифікатор, що буде на основі вхідних даних обирати клас алгоритмів (деяке значення Т), для яких варто проводити побудову моделей. Іншим варіантом може бути підтримка структури у вигляді словника, що буде зберігати наперед визначені користувачем набори типу "критерій"-"клас алгоритмів" і значно швидше (за лінійний час) буде обирати потрібну множину алгоритмів. Останній підхід є значно швидшим, але вимагає додаткової початкової ініціалізації та втручання людини для підготовки такого словника.
	\item Запуск побудови кожної моделі в окремому потоці. Процес побудови моделі є процесом, що в першу чергу вимагає процесорний час для виконання (\textit{cpu-bound}), тому з апаратної точки зору прискорити його роботу можливо за рахунок розпаралелювання на декількох ядрах процесора. Найпростішим варіантом є використання багатоядерних процесорів і виконнання алгоритму на окремому ядрі. Сучасні відеокарти з підтримкою технологій \textit{Nvidia CUDA} та \textit{AMD OpenCL} теж можуть бути використані для запуску даних алгоритмів. Порівняння показують, що використання відеокарт для схожих обрахунків може надати приріст у розмірі 90-95х кратного прискорення роботи. Аналогічним чином можна скористатися розподіленими та багатопроцесрними системами, коли алгоритми будуть виконуватися окреми на різних машинах в межах одного кластеру. Головним недоліком таких систем є підвищення порогу входження для розробки, адже це вимагає додаткових знань як для написання коду (\textit{C++, MPI, OpenMPI}), так і розуміння архітектури розподілених систем вцілому. Наприклад, для написання алгоритму для кластеру потрібно розуміти, що кожна модель на окремій ноді кластеру повинна мати доступ до вхідного датасету, а отже потрібно забезпечити централізований неблокуючий доступ до даних або реалізувати можливість спільної пам'яті (\textit{shared memory}), що теж вимагає додаткових зусиль з точки зору програміста. З переваг варто відмітити однократність даної операції та практично необмежений лінійний ріст ефективності пропорційно до кількості початкових алгоритмів.
	\item Під час сумісної роботи над проектом виникає необхідність обміну даними між розробниками. Науковці хочуть мати змогу надсилати моделі іншим, а також мати змогу їх зберегти для подальшого використання. Тому ще одним можливим шляхом для вдосконалення може бути можливість серіалізації моделей. Таким чином модель можна буде побудувати і зберегти в бінарному форматі на одному комп'ютері, а потім використати в майбутньому без необхідності повторної її перебудови. Звісно такий підхід працює лише для одного набору вхідних даних, тому область його застосування досить обмежена. Проте, якщо над деякими даними працює команда науковців, саме це дасть змогу швидко обмінюватися результатами чи використовувати напрацювання інших. Найпростішою реалізаціює тут може бути знімок об'єкта у пам'яті, але таким чином втрачається кросплатформенність та машинонезалежність. Вбудовані підходи до серіалізації (наприклад, \textit{pickle}) також будуть страждати від подібних нюансів. Саме тому необхідно буде розробити додатковий алгоритм для серіалізації моделей, що і є головним фактором, який стримує додавання даної можливості до існуючого коду.
\end{itemize}

Розглянуті напрямки покращення дозволять збільшити швидкодію програми вцілому, а також спростити використання її в межах команди науковців, а тому доцільно розглянути подальшу роботу над проектом саме в одному із запропонованих напрямків.